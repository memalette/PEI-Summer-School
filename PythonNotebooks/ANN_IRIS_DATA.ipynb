{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to import\n",
    "# The 'sklearn' libraries are only to use the Cross Validation method\n",
    "# The 'keras' libraries are used for the neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.wrappers.scikit_learn import BaseWrapper\n",
    "import sklearn.metrics as sm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_len</th>\n",
       "      <th>sepal_wid</th>\n",
       "      <th>petal_len</th>\n",
       "      <th>petal_wid</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_len  sepal_wid  petal_len  petal_wid           class\n",
       "145        6.7        3.0        5.2        2.3  Iris-virginica\n",
       "146        6.3        2.5        5.0        1.9  Iris-virginica\n",
       "147        6.5        3.0        5.2        2.0  Iris-virginica\n",
       "148        6.2        3.4        5.4        2.3  Iris-virginica\n",
       "149        5.9        3.0        5.1        1.8  Iris-virginica"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "\n",
    "dataset.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "dataset.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\n",
    "\n",
    "dataset.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframe to values\n",
    "X=dataset.values\n",
    "#X\n",
    "#X[inds[:(n_train+n_test)],0:4]\n",
    "#X[inds[:(n_train+n_test)],4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nbs of train, test and valid data points\n",
    "n_train = 50\n",
    "n_test = 50\n",
    "n_valid = 50\n",
    "\n",
    "# Random shuffle\n",
    "seed = 10\n",
    "np.random.seed(10)\n",
    "inds = np.arange(dataset.shape[0])\n",
    "np.random.shuffle(inds)\n",
    "X_train_test = X[inds[:(n_train+n_test)],0:4]\n",
    "Y_train_test = X[inds[:(n_train+n_test)],4]\n",
    "X_valid = X[inds[(n_train+n_test):150],0:4]\n",
    "Y_valid = X[inds[(n_train+n_test):150],4]\n",
    "#X_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 87, 111,  10,  91,  49,  60,  72,  67,  39,  55,  66, 142,  53,\n",
       "         1,  19, 112,  85,  38,  21,  35, 102, 132, 126,  24,  61,   2,\n",
       "        95,  90,  76, 117,  58,  97, 129, 114, 146,  47, 124, 120, 118,\n",
       "       141,  26,  43,  59,  41,  56,  32,  52,  70, 121, 144,  68, 109,\n",
       "        81,  78,  51,  14,  48,  63,  20, 137,  29,   3, 106,  98, 140,\n",
       "        37, 139,  45,  82,  83,  42, 143, 131, 116, 105, 133,   6,  79,\n",
       "       108,  99,  34, 128,  50,  96,   4,   5,  44,  84, 130, 103,  75,\n",
       "         7,  46,  17, 104, 101,  71,  80, 110, 147, 149, 134,  74,  28,\n",
       "        11,  94,  23,  22, 127,  93,  18,  27,  36,  57,  31,  65,  12,\n",
       "        89, 119,  30,  86,  92, 148,  25, 138,  13,  69,  77, 135, 136,\n",
       "        33,  62, 122, 107,  88,  54, 100,  16, 115,  40,   0,  73,   8,\n",
       "       145, 123, 113,  64,  15, 125,   9])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train_test)\n",
    "encoded_Y = encoder.transform(Y_train_test)\n",
    "dummy_y_train_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 1, 1, 2,\n",
       "       0, 2, 0, 1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2,\n",
       "       1, 0, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_valid)\n",
    "encoded_Y_valid = encoder.transform(Y_valid)\n",
    "encoded_Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummy_y_train_test\n",
    "#encoded_Y\n",
    "X_train_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the neural network model - 1 hidden layer\n",
    "# In the definition of the function, we set \"input_dim\", \"output_dim\" and \"activation\" to None (see next box)\n",
    "# The reason is that we will input those parameters in the function\n",
    "def baseline_model(input_dim=None, output_dim=None, activation=None):\n",
    "    \n",
    "    # 1) Create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 2) Input layer:\n",
    "    # input_dim  : dimension of the features\n",
    "    # output_dim : number of hidden neurons\n",
    "    # activation : function \"h_{2}\" in the PDF. Here, we will use 'relu'\n",
    "    # init is always at \"normal\"\n",
    "    model.add(Dense(output_dim = output_dim, input_dim = input_dim, init = 'normal', activation = 'relu'))\n",
    "    \n",
    "    # 3) Output layer:\n",
    "    # The output dim is \"3\" because there's 3 categories\n",
    "    # Init is always at \"normal\"\n",
    "    model.add(Dense(output_dim = 3, init = 'normal', activation = 'softmax'))\n",
    "    \n",
    "    # 4) Compile model\n",
    "    # The loss function for the classification problem is called 'categorical_crossentropy'\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.evaluate(X_train_test, dummy_y_train_test,batch_size=32)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.00% (12.69%)\n"
     ]
    }
   ],
   "source": [
    "# Calls the function \"baseline_model\" with the inputs needed\n",
    "esti = KerasClassifier(build_fn=baseline_model, nb_epoch = 100, batch_size=32, verbose=0, input_dim = X_train_test.shape[1], output_dim = 15)\n",
    "#estimator = KerasClassifier(build_fn=baseline_model, nb_epoch = nb_epoch[j], batch_size=32, verbose=0, input_dim = X.shape[1], output_dim = d_out[i])\n",
    "# Use the K-Fold cross validation technique with K = 10\n",
    "#kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#kfold\n",
    "#.evaluate(X_train_test, dummy_y_train_test,batch_size=32)\n",
    "results = cross_val_score(esti, X_train_test, dummy_y_train_test, cv=kfold)\n",
    "#kfold.split(X_train_test,dummy_y_train_test)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.00% (10.25%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This does the hyperparameter selection\n",
    "import warnings #So that it doesn't show a warning message\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Optimization process - step 2 to 5 in the PDF\n",
    "d_out = np.arange(start=10, stop=30, step=2)      # Grid of hidden neurons\n",
    "nb_epoch = np.arange(start=30,stop=130,step=10)    # Grid of the number of epochs\n",
    "batch_size = 32                   # Batch size for the gradient descent : see description link in PDF\n",
    "#output_dim = 3                    # Dimension of the output is the number of classes, 3 in our case\n",
    "input_dim = X_train_test.shape[0] # Dimension of the initial input (100 data points in our case)\n",
    "results_opt = 0                   # Variable to stock the best results\n",
    "\n",
    "# Loop for the first hyperparameter : nbs of hidden neurons\n",
    "for i in range(len(d_out)):\n",
    "    \n",
    "    # Loop for the second hyperparameter : nbs of epochs\n",
    "    for j in range(len(nb_epoch)):\n",
    "        \n",
    "        # Calls the function \"baseline_model\" with the inputs needed\n",
    "        esti = KerasClassifier(build_fn=baseline_model, nb_epoch = nb_epoch[j], batch_size=32, verbose=0, input_dim = X_train_test.shape[1], output_dim = d_out[i])\n",
    "        #estimator = KerasClassifier(build_fn=baseline_model, nb_epoch = nb_epoch[j], batch_size=32, verbose=0, input_dim = X.shape[1], output_dim = d_out[i])\n",
    "        # Use the K-Fold cross validation technique with K = 10\n",
    "        kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "        \n",
    "        # Calculate the % of classification success for each of the K-Fold combinaison\n",
    "        results = cross_val_score(esti, X_train_test, dummy_y_train_test, cv=kfold)\n",
    "        \n",
    "        # Check if we get better results than our last best\n",
    "        if(results.mean()*100 > results_opt):\n",
    "            d_out_opt = d_out[i]\n",
    "            nb_epoch_opt = nb_epoch[j]\n",
    "            results_opt = results.mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.9999996424\n",
      "20\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "# Results on the Test set with the K-fold method as well as the optimal nbs of hidden neurons and hidden layer\n",
    "print(results_opt)\n",
    "print(d_out_opt)\n",
    "print(nb_epoch_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/110\n",
      "100/100 [==============================] - 0s - loss: 1.0832 - acc: 0.3500     \n",
      "Epoch 2/110\n",
      "100/100 [==============================] - 0s - loss: 1.0791 - acc: 0.3500     \n",
      "Epoch 3/110\n",
      "100/100 [==============================] - 0s - loss: 1.0759 - acc: 0.3500     \n",
      "Epoch 4/110\n",
      "100/100 [==============================] - 0s - loss: 1.0726 - acc: 0.3500     \n",
      "Epoch 5/110\n",
      "100/100 [==============================] - 0s - loss: 1.0696 - acc: 0.3600     \n",
      "Epoch 6/110\n",
      "100/100 [==============================] - 0s - loss: 1.0671 - acc: 0.5700     \n",
      "Epoch 7/110\n",
      "100/100 [==============================] - 0s - loss: 1.0642 - acc: 0.4200     \n",
      "Epoch 8/110\n",
      "100/100 [==============================] - 0s - loss: 1.0611 - acc: 0.5000     \n",
      "Epoch 9/110\n",
      "100/100 [==============================] - 0s - loss: 1.0578 - acc: 0.6000     \n",
      "Epoch 10/110\n",
      "100/100 [==============================] - 0s - loss: 1.0549 - acc: 0.4500     \n",
      "Epoch 11/110\n",
      "100/100 [==============================] - 0s - loss: 1.0523 - acc: 0.4000     \n",
      "Epoch 12/110\n",
      "100/100 [==============================] - 0s - loss: 1.0493 - acc: 0.5400     \n",
      "Epoch 13/110\n",
      "100/100 [==============================] - 0s - loss: 1.0468 - acc: 0.6100     \n",
      "Epoch 14/110\n",
      "100/100 [==============================] - 0s - loss: 1.0440 - acc: 0.6600     \n",
      "Epoch 15/110\n",
      "100/100 [==============================] - 0s - loss: 1.0410 - acc: 0.6600     \n",
      "Epoch 16/110\n",
      "100/100 [==============================] - 0s - loss: 1.0373 - acc: 0.6200     \n",
      "Epoch 17/110\n",
      "100/100 [==============================] - 0s - loss: 1.0332 - acc: 0.6100     \n",
      "Epoch 18/110\n",
      "100/100 [==============================] - 0s - loss: 1.0293 - acc: 0.5900     \n",
      "Epoch 19/110\n",
      "100/100 [==============================] - 0s - loss: 1.0251 - acc: 0.5700     \n",
      "Epoch 20/110\n",
      "100/100 [==============================] - 0s - loss: 1.0209 - acc: 0.4300     \n",
      "Epoch 21/110\n",
      "100/100 [==============================] - 0s - loss: 1.0170 - acc: 0.4100     \n",
      "Epoch 22/110\n",
      "100/100 [==============================] - 0s - loss: 1.0129 - acc: 0.5000     \n",
      "Epoch 23/110\n",
      "100/100 [==============================] - 0s - loss: 1.0085 - acc: 0.5800     \n",
      "Epoch 24/110\n",
      "100/100 [==============================] - 0s - loss: 1.0039 - acc: 0.6700     \n",
      "Epoch 25/110\n",
      "100/100 [==============================] - 0s - loss: 0.9994 - acc: 0.6800     \n",
      "Epoch 26/110\n",
      "100/100 [==============================] - 0s - loss: 0.9946 - acc: 0.6800     \n",
      "Epoch 27/110\n",
      "100/100 [==============================] - 0s - loss: 0.9900 - acc: 0.7100     \n",
      "Epoch 28/110\n",
      "100/100 [==============================] - 0s - loss: 0.9850 - acc: 0.7100     \n",
      "Epoch 29/110\n",
      "100/100 [==============================] - 0s - loss: 0.9795 - acc: 0.7000     \n",
      "Epoch 30/110\n",
      "100/100 [==============================] - 0s - loss: 0.9743 - acc: 0.7100     \n",
      "Epoch 31/110\n",
      "100/100 [==============================] - 0s - loss: 0.9689 - acc: 0.7800     \n",
      "Epoch 32/110\n",
      "100/100 [==============================] - 0s - loss: 0.9634 - acc: 0.8600     \n",
      "Epoch 33/110\n",
      "100/100 [==============================] - 0s - loss: 0.9575 - acc: 0.9400     \n",
      "Epoch 34/110\n",
      "100/100 [==============================] - 0s - loss: 0.9514 - acc: 0.9600     \n",
      "Epoch 35/110\n",
      "100/100 [==============================] - 0s - loss: 0.9453 - acc: 0.9700     \n",
      "Epoch 36/110\n",
      "100/100 [==============================] - 0s - loss: 0.9390 - acc: 0.9500     \n",
      "Epoch 37/110\n",
      "100/100 [==============================] - 0s - loss: 0.9329 - acc: 0.8900     \n",
      "Epoch 38/110\n",
      "100/100 [==============================] - 0s - loss: 0.9263 - acc: 0.7900     \n",
      "Epoch 39/110\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.9230 - acc: 0.750 - 0s - loss: 0.9196 - acc: 0.7900     \n",
      "Epoch 40/110\n",
      "100/100 [==============================] - 0s - loss: 0.9129 - acc: 0.8300     \n",
      "Epoch 41/110\n",
      "100/100 [==============================] - 0s - loss: 0.9063 - acc: 0.8700     \n",
      "Epoch 42/110\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.9138 - acc: 0.937 - 0s - loss: 0.8991 - acc: 0.9600     \n",
      "Epoch 43/110\n",
      "100/100 [==============================] - 0s - loss: 0.8917 - acc: 0.9600     \n",
      "Epoch 44/110\n",
      "100/100 [==============================] - 0s - loss: 0.8847 - acc: 0.9600     \n",
      "Epoch 45/110\n",
      "100/100 [==============================] - 0s - loss: 0.8771 - acc: 0.9500     \n",
      "Epoch 46/110\n",
      "100/100 [==============================] - 0s - loss: 0.8697 - acc: 0.9600     \n",
      "Epoch 47/110\n",
      "100/100 [==============================] - 0s - loss: 0.8624 - acc: 0.9700     \n",
      "Epoch 48/110\n",
      "100/100 [==============================] - 0s - loss: 0.8545 - acc: 0.9600     \n",
      "Epoch 49/110\n",
      "100/100 [==============================] - 0s - loss: 0.8468 - acc: 0.9600     \n",
      "Epoch 50/110\n",
      "100/100 [==============================] - 0s - loss: 0.8392 - acc: 0.9800     \n",
      "Epoch 51/110\n",
      "100/100 [==============================] - 0s - loss: 0.8317 - acc: 0.9600     \n",
      "Epoch 52/110\n",
      "100/100 [==============================] - 0s - loss: 0.8241 - acc: 0.9400     \n",
      "Epoch 53/110\n",
      "100/100 [==============================] - 0s - loss: 0.8172 - acc: 0.8500     \n",
      "Epoch 54/110\n",
      "100/100 [==============================] - 0s - loss: 0.8100 - acc: 0.7800     \n",
      "Epoch 55/110\n",
      "100/100 [==============================] - 0s - loss: 0.8022 - acc: 0.7300     \n",
      "Epoch 56/110\n",
      "100/100 [==============================] - 0s - loss: 0.7942 - acc: 0.7300     \n",
      "Epoch 57/110\n",
      "100/100 [==============================] - 0s - loss: 0.7865 - acc: 0.7100     \n",
      "Epoch 58/110\n",
      "100/100 [==============================] - 0s - loss: 0.7794 - acc: 0.6900     \n",
      "Epoch 59/110\n",
      "100/100 [==============================] - 0s - loss: 0.7722 - acc: 0.6800     \n",
      "Epoch 60/110\n",
      "100/100 [==============================] - 0s - loss: 0.7643 - acc: 0.6800     \n",
      "Epoch 61/110\n",
      "100/100 [==============================] - 0s - loss: 0.7572 - acc: 0.6900     \n",
      "Epoch 62/110\n",
      "100/100 [==============================] - 0s - loss: 0.7503 - acc: 0.6900     \n",
      "Epoch 63/110\n",
      "100/100 [==============================] - 0s - loss: 0.7430 - acc: 0.6900     \n",
      "Epoch 64/110\n",
      "100/100 [==============================] - 0s - loss: 0.7361 - acc: 0.6900     \n",
      "Epoch 65/110\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6973 - acc: 0.781 - 0s - loss: 0.7292 - acc: 0.7000     \n",
      "Epoch 66/110\n",
      "100/100 [==============================] - 0s - loss: 0.7227 - acc: 0.6900     \n",
      "Epoch 67/110\n",
      "100/100 [==============================] - 0s - loss: 0.7162 - acc: 0.6900     \n",
      "Epoch 68/110\n",
      "100/100 [==============================] - 0s - loss: 0.7098 - acc: 0.7100     \n",
      "Epoch 69/110\n",
      "100/100 [==============================] - 0s - loss: 0.7033 - acc: 0.7200     \n",
      "Epoch 70/110\n",
      "100/100 [==============================] - 0s - loss: 0.6969 - acc: 0.7300     \n",
      "Epoch 71/110\n",
      "100/100 [==============================] - 0s - loss: 0.6910 - acc: 0.7700     \n",
      "Epoch 72/110\n",
      "100/100 [==============================] - 0s - loss: 0.6849 - acc: 0.8000     \n",
      "Epoch 73/110\n",
      "100/100 [==============================] - 0s - loss: 0.6789 - acc: 0.8500     \n",
      "Epoch 74/110\n",
      "100/100 [==============================] - 0s - loss: 0.6734 - acc: 0.8800     \n",
      "Epoch 75/110\n",
      "100/100 [==============================] - 0s - loss: 0.6679 - acc: 0.9200     \n",
      "Epoch 76/110\n",
      "100/100 [==============================] - 0s - loss: 0.6622 - acc: 0.9200     \n",
      "Epoch 77/110\n",
      "100/100 [==============================] - 0s - loss: 0.6568 - acc: 0.8500     \n",
      "Epoch 78/110\n",
      "100/100 [==============================] - 0s - loss: 0.6526 - acc: 0.8000     \n",
      "Epoch 79/110\n",
      "100/100 [==============================] - 0s - loss: 0.6472 - acc: 0.7600     \n",
      "Epoch 80/110\n",
      "100/100 [==============================] - 0s - loss: 0.6427 - acc: 0.7600     \n",
      "Epoch 81/110\n",
      "100/100 [==============================] - 0s - loss: 0.6380 - acc: 0.8000     \n",
      "Epoch 82/110\n",
      "100/100 [==============================] - 0s - loss: 0.6335 - acc: 0.8000     \n",
      "Epoch 83/110\n",
      "100/100 [==============================] - 0s - loss: 0.6289 - acc: 0.8000     \n",
      "Epoch 84/110\n",
      "100/100 [==============================] - 0s - loss: 0.6243 - acc: 0.8000     \n",
      "Epoch 85/110\n",
      "100/100 [==============================] - 0s - loss: 0.6196 - acc: 0.8100     \n",
      "Epoch 86/110\n",
      "100/100 [==============================] - 0s - loss: 0.6150 - acc: 0.8300     \n",
      "Epoch 87/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s - loss: 0.6106 - acc: 0.8600     \n",
      "Epoch 88/110\n",
      "100/100 [==============================] - 0s - loss: 0.6068 - acc: 0.8800     \n",
      "Epoch 89/110\n",
      "100/100 [==============================] - 0s - loss: 0.6026 - acc: 0.9200     \n",
      "Epoch 90/110\n",
      "100/100 [==============================] - 0s - loss: 0.5983 - acc: 0.9700     \n",
      "Epoch 91/110\n",
      "100/100 [==============================] - 0s - loss: 0.5944 - acc: 0.9300     \n",
      "Epoch 92/110\n",
      "100/100 [==============================] - 0s - loss: 0.5905 - acc: 0.8900     \n",
      "Epoch 93/110\n",
      "100/100 [==============================] - 0s - loss: 0.5871 - acc: 0.8600     \n",
      "Epoch 94/110\n",
      "100/100 [==============================] - 0s - loss: 0.5839 - acc: 0.8200     \n",
      "Epoch 95/110\n",
      "100/100 [==============================] - 0s - loss: 0.5803 - acc: 0.8100     \n",
      "Epoch 96/110\n",
      "100/100 [==============================] - 0s - loss: 0.5766 - acc: 0.8100     \n",
      "Epoch 97/110\n",
      "100/100 [==============================] - 0s - loss: 0.5733 - acc: 0.8200     \n",
      "Epoch 98/110\n",
      "100/100 [==============================] - 0s - loss: 0.5699 - acc: 0.8200     \n",
      "Epoch 99/110\n",
      "100/100 [==============================] - 0s - loss: 0.5670 - acc: 0.8100     \n",
      "Epoch 100/110\n",
      "100/100 [==============================] - 0s - loss: 0.5638 - acc: 0.8100     \n",
      "Epoch 101/110\n",
      "100/100 [==============================] - 0s - loss: 0.5607 - acc: 0.8300     \n",
      "Epoch 102/110\n",
      "100/100 [==============================] - 0s - loss: 0.5574 - acc: 0.8500     \n",
      "Epoch 103/110\n",
      "100/100 [==============================] - 0s - loss: 0.5544 - acc: 0.8800     \n",
      "Epoch 104/110\n",
      "100/100 [==============================] - 0s - loss: 0.5517 - acc: 0.9200     \n",
      "Epoch 105/110\n",
      "100/100 [==============================] - 0s - loss: 0.5482 - acc: 0.9700     \n",
      "Epoch 106/110\n",
      "100/100 [==============================] - 0s - loss: 0.5458 - acc: 0.9700     \n",
      "Epoch 107/110\n",
      "100/100 [==============================] - 0s - loss: 0.5428 - acc: 0.9700     \n",
      "Epoch 108/110\n",
      "100/100 [==============================] - 0s - loss: 0.5400 - acc: 0.9700     \n",
      "Epoch 109/110\n",
      "100/100 [==============================] - 0s - loss: 0.5375 - acc: 0.9800     \n",
      "Epoch 110/110\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5130 - acc: 0.968 - 0s - loss: 0.5349 - acc: 0.9700     \n"
     ]
    }
   ],
   "source": [
    "# Step 6 : Retrain our parameters on the Train AND Test set, i.e. a new Neural Network model\n",
    "# d_out_opt : optimal number of hidden neurons\n",
    "# nb_epoch_opt : optimal number of epochs\n",
    "input_dim = X_train_test.shape[1]\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(output_dim = 20, input_dim = input_dim, activation='sigmoid'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(output_dim = 3, init = 'normal', activation = 'softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# We don't use the cross-validation here, we only train the model on the Train&Test set.\n",
    "# To do so, we use the function \"model.fit\"\n",
    "logs = model.fit(X_train_test, dummy_y_train_test, nb_epoch = 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 1, 1, 2,\n",
       "       0, 2, 0, 1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 2, 0, 2, 0, 0, 1, 0, 2, 2, 2,\n",
       "       1, 0, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.predict(X_valid)\n",
    "#np.argmax(model.predict(X_valid), axis=1)\n",
    "encoded_Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the rate of classification success on the Valid set\n",
    "# model.predict(X_valid) uses the function Softmax, i.e. returns a vector of dimension that sums to 1 \n",
    "# for each X vector. The prediction is the highest value of each vector (use np.argmax)\n",
    "pred_valid_set = np.argmax(model.predict(X_valid), axis=1)\n",
    "sm.accuracy_score(pred_valid_set, encoded_Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model predictions of the Validation set\n",
    "pred_valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Real classification of the Validation set\n",
    "Y_valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
